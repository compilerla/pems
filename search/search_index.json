{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>As of October 2025, this project has ended and the repository is now archived.</p>"},{"location":"#pems","title":"PeMS","text":"<p>Caltrans Performance Measurement System</p> <p>PeMS is open-source software that is designed, developed, and maintained by Compiler LLC on behalf of Caltrans Traffic Operations.</p>"},{"location":"#product-roadmap","title":"Product Roadmap","text":"<p>Our product roadmap captures what we\u2019re currently building, what we\u2019ve built, and what we plan to build in future quarters. It is updated on a regular basis, aligned with project progress.</p> <pre><code>timeline\n    title PeMS Product Roadmap\n    %% PeMS Epics (2024)\n    section 2024\n\n    Q3&lt;br&gt;Complete\n    : Project launch\n    : Established resources and overall scope\n    : Began discovery work\n\n    Q4&lt;br&gt;Complete\n    : Confirm site architecture\n    : Set up GitHub repository\n    : Scaffold app structure\n    : Create a prioritized backlog of features\n    : Created local environment\n\n    %% PeMS Epics (2025)\n    section 2025\n\n    Q1&lt;br&gt;Complete\n    : Project paused for AWS license approval\n\n    Q2&lt;br&gt;Now\n    : Create dev environment\n    : Create test environment\n    : Launch test version of district-specific pages\n    : Set up a working CI/CD pipeline\n\n    Q3&lt;br&gt;Planned\n    : Create production environment\n    : User testing for district-specific pages and initial features (early Q3)\n    : Launch test version of full site\n    : Additional user testing for full site (timing TBD)\n\n    Q4&lt;br&gt;Future\n    : Go-Live - Date TBD\n    : Compiler contract for PeMS ends September 2025, but planning to extend to Dec. 2025\n\n    %%{\n        init: {\n            'logLevel': 'debug',\n            'theme': 'default' ,\n            'themeVariables': {\n                'cScale0': '#ffa500', 'cScaleLabel0': '#000000',\n                'cScale1': '#ffff00', 'cScaleLabel1': '#000000',\n                'cScale2': '#ffff00', 'cScaleLabel2': '#000000',\n                'cScale3': '#008000', 'cScaleLabel3': '#ffffff',\n                'cScale4': '#0000ff', 'cScaleLabel4': '#ffffff',\n                'cScale5': '#4b0082', 'cScaleLabel5': '#ffffff',\n                'cScale6': '#000000', 'cScaleLabel6': '#ffffff'\n            }\n        }\n    }%%</code></pre>"},{"location":"deployment/hotfix/","title":"Making a hotfix release","text":"<p>This list outlines the manual steps needed to make a hotfix release of the <code>pems</code> app.</p> <p>If <code>main</code> contains in-progress work that is not yet ready for a release but a simple code fix is needed in production, a separate process to test the changes before deploying to production must be undertaken. This is called a hotfix release. Typically, a hotfix release involves a simple code change that can be quickly implemented, in contrast to a rollback release, which generally requires more complex code changes which take more time to implement. To coordinate the work that\u2019s required for a hotfix release, a <code>Release process Issue</code> needs to be created. The button below will help you start a new <code>Release process Issue</code> by using an Issue template.</p> <p>Start a new Release on Github</p>"},{"location":"deployment/hotfix/#0-create-a-temporary-hotfix-branch-from-the-latest-release-tag","title":"0. Create a temporary hotfix branch from the latest release tag","text":"<pre><code>git checkout -b &lt;hotfix-branch&gt; &lt;release-tag&gt;\n</code></pre> <p>Replace <code>&lt;hotfix-branch&gt;</code> with the hotfix branch name and <code>&lt;release-tag&gt;</code> with the latest release tag.</p>"},{"location":"deployment/hotfix/#1-fix-whatever-issue-is-wrong-using-the-hotfix-branch","title":"1. Fix whatever issue is wrong using the hotfix branch","text":"<p>Commit the code changes that fix the issue that prompted the hotfix.</p>"},{"location":"deployment/hotfix/#2-tag-the-head-of-the-hotfix-branch-with-a-release-tag","title":"2. Tag the HEAD of the hotfix branch with a release tag","text":"<pre><code>git tag -a YYYY.0M.R\n</code></pre> <p>Git will open your default text editor and prompt you for the tag annotation. For the tag annotation, use the release tag version and close the text editor.</p>"},{"location":"deployment/hotfix/#3-push-the-tag-to-github-to-kick-off-the-hotfix","title":"3. Push the tag to GitHub to kick off the hotfix","text":"<pre><code>git push origin YYYY.0M.R\n</code></pre>"},{"location":"deployment/hotfix/#4-generate-release-notes","title":"4. Generate release notes","text":"<p>Edit release notes with additional context, images, animations, etc. as-needed and link to the <code>Release process Issue</code>.</p>"},{"location":"deployment/hotfix/#5-merge-into-main-for-the-next-release","title":"5. Merge into <code>main</code> for the next release","text":"<p>Create a PR to merge the changes from the hotfix branch into <code>main</code> for the next release.</p>"},{"location":"deployment/release/","title":"Making a release","text":"<p>This list outlines the manual steps needed to make a new release of the <code>pems</code> app.</p> <p>A release is made by pushing an annotated tag that is named after the version number for the app and the release. The name of the tag must use the version number format mentioned below. Pushing an annotated tag kicks off a deployment (implemented as a GitHub Actions workflow) that builds, tags, and pushes the app\u2019s image to the GitHub Container Registry and then creates a GitHub release. It is often useful to monitor the release process by looking at the status of the Deploy workflow under the <code>Actions</code> section of the repository.</p> <p>The list of releases can be found on the repository\u2019s Releases page on GitHub.</p>"},{"location":"deployment/release/#0-decide-on-the-new-version-number-and-create-a-release-process-issue","title":"0. Decide on the new version number and create a <code>Release process Issue</code>","text":"<p>A new release implies a new version.</p> <p><code>pems</code> uses the CalVer versioning scheme, where version numbers look like: <code>YYYY.0M.R</code></p> <ul> <li><code>YYYY</code> is the 4-digit year of the release; e.g. <code>2024</code>, <code>2025</code></li> <li><code>0M</code> is the 2-digit, 0-padded month of the release; e.g. <code>02</code> is February, <code>12</code>   is December.</li> <li><code>R</code> is the 1-based release counter for the given year and month;   e.g. <code>1</code> for the first release of the month, <code>2</code> for the second, and so on.</li> </ul> <p>Version numbers for release candidates append <code>-rcR</code>, where <code>R</code> is the 1-based release counter for the anticipated release. For example, the first release candidate for the <code>2025.01.1</code> release would be <code>2025.01.1-rc1</code>.</p> <p>To coordinate the work that\u2019s required for a release, a <code>Release process Issue</code> needs to be created. The button below will help you start a new <code>Release process Issue</code> by using an Issue template.</p> <p>Start a new Release on Github</p>"},{"location":"deployment/release/#1-create-a-release-candidate-tag-on-main-and-push-it","title":"1. Create a release candidate tag on <code>main</code> and push it","text":"<pre><code>git fetch\ngit checkout main\ngit reset --hard origin/main\ngit tag -a YYYY.0M.R-rcR\n</code></pre> <p>Git will open your default text editor and prompt you for the tag annotation. For the tag annotation, use <code>Release candidate R for YYYY.0M.R</code>. For example, <code>Release candidate 2 for 2025.01.1</code> would be the annotation for the second release candidate of the first release of January 2025. Finally, after closing the text editor:</p> <pre><code>git push origin YYYY.0M.R-rcR\n</code></pre> <p>This builds a new package, tags it, and pushes the app\u2019s image to GitHub Container Registry. No GitHub release is created for release candidates.</p>"},{"location":"deployment/release/#2-create-a-release-tag-on-main-and-push-it","title":"2. Create a release tag on <code>main</code> and push it","text":"<pre><code>git fetch\ngit checkout main\ngit reset --hard origin/main\ngit tag -a YYYY.0M.R\n</code></pre> <p>Git will open your default text editor and prompt you for the tag annotation. For the tag annotation, use the title of the Release process issue that kicked off the release. Finally, after closing the text editor:</p> <pre><code>git push origin YYYY.0M.R\n</code></pre> <p>This builds the package, tags it, pushes the app\u2019s image to GitHub Container Registry, and creates a GitHub release.</p>"},{"location":"deployment/release/#3-generate-release-notes","title":"3. Generate release notes","text":"<p>Edit release notes with additional context, images, animations, etc. as-needed and link to the <code>Release process Issue</code>.</p>"},{"location":"deployment/rollback/","title":"Making a rollback release","text":"<p>This list outlines the manual steps needed to make a rollback of the <code>pems</code> app.</p> <p>If a change is deployed to the app that makes it fail to start, making a rollback will deploy the app to a known working state again. To coordinate the work that\u2019s required for a rollback release, a <code>Release process Issue</code> needs to be created. The button below will help you start a new <code>Release process Issue</code> by using an Issue template.</p> <p>Start a new Release on Github</p>"},{"location":"deployment/rollback/#0-create-a-release-tag-on-the-commit-associated-with-the-last-known-good-release-tag","title":"0. Create a release tag on the commit associated with the last known good release tag","text":"<pre><code>git tag -a YYYY.0M.R &lt;commit-hash&gt;\n</code></pre> <p>Replace <code>YYYY.0M.R</code> with the rollback version and <code>&lt;commit-hash&gt;</code> with the hash of the commit associated with the last known good release tag. Git will open your default text editor and prompt you for the tag annotation. For the tag annotation, use the version of the release tag for the rollback and close the text editor.</p>"},{"location":"deployment/rollback/#1-push-the-tag-to-github-to-kick-off-the-rollback","title":"1. Push the tag to GitHub to kick off the rollback","text":"<pre><code>git push origin YYYY.0M.R\n</code></pre>"},{"location":"deployment/rollback/#2-generate-release-notes","title":"2. Generate release notes","text":"<p>Edit release notes with additional context, images, animations, etc. as-needed and link to the <code>Release process Issue</code>.</p>"},{"location":"development/","title":"Getting started with development","text":"<p>Info</p> <p>This guide will take you through the process of getting the <code>pems</code> project running in your local development environment.</p> <p><code>pems</code> uses VS Code devcontainers to provide a platform-agnostic, standardized development environment.</p> <p>For more about why we use Dev Containers, check our Compiler\u2019s blog post: How to support a platform-agnostic engineering team with VS Code Dev Containers.</p>"},{"location":"development/#prerequisites","title":"Prerequisites","text":"<p>This section describes the tooling you need to have installed and configured on your development machine before continuing.</p>"},{"location":"development/#git","title":"Git","text":"<p>Git is an open source version control system that we use in <code>pems</code> to track changes to the codebase over time. Many operating systems come with Git already installed. Check if you have Git installed in a terminal with the following command:</p> <pre><code>git --version\n</code></pre> <p>If git is installed, the output should look similar to:</p> <pre><code>$ git --version\ngit version 2.39.5\n</code></pre> <p>If Git is not installed, head to the Git downloads page to get an installer for your operating system.</p>"},{"location":"development/#docker-and-docker-compose","title":"Docker and Docker Compose","text":"<p>Docker and Docker Compose (or just Compose) are key tools that allow for running the various services required for <code>pems</code>.</p> <p>Confirm if you already have Docker installed, in a terminal:</p> <pre><code>docker --version\n</code></pre> <p>If Docker is installed, the output should look similar to:</p> <pre><code>$ docker --version\nDocker version 27.4.0, build bde2b89\n</code></pre> <p>And similarly to check if Compose is installed:</p> <pre><code>docker compose version\n</code></pre> <p>When Compose is installed, output will look similar to:</p> <pre><code>$ docker compose version\nDocker Compose version v2.31.0\n</code></pre> <p>There are different ways to acquire this software depending on your operating system. The simplest approach for Windows and MacOS users is to install Docker Desktop.</p> License requirements for Docker Desktop <p>Use of Docker Desktop is subject to Docker\u2019s licensing terms. In particular, note that Section 4.2 calls out government users specifically:</p> <p>Government Entities shall not use Docker Desktop or access other Entitlements of the Service without purchasing a Subscription.</p>"},{"location":"development/#windows","title":"Windows","text":"<p>It is possible to run Docker and Compose on Windows without installing Docker Desktop. This involves using the Windows Subsystem for Linux v2 (WSL2), where Docker is configured to run.</p> <p>This article walks through this procedure in more detail: How to run docker on Windows without Docker Desktop.</p>"},{"location":"development/#macos","title":"MacOS","text":"<p>With MacOS and Homebrew, installing Docker and Compose are as simple as:</p> <pre><code>brew install docker docker-compose colima\n</code></pre> <p>Once the install completes, start <code>colima</code> (an open source container runtime):</p> <pre><code>brew services start colima\n</code></pre>"},{"location":"development/#linux","title":"Linux","text":"<p>Docker CE (also known as Docker Engine) is how to run Docker and Compose on Linux. Docker provides an installation guide for Docker CE.</p>"},{"location":"development/#vs-code-and-dev-containers-extension","title":"VS Code and Dev Containers extension","text":"<p>VS Code is an open source Integrated Development Environment (IDE) from Microsoft. Check if you already have it installed:</p> <pre><code>code -v\n</code></pre> <p>If installed, output should look similar to:</p> <pre><code>$ code -v\n1.95.3\nf1a4fb101478ce6ec82fe9627c43efbf9e98c813\nx64\n</code></pre> <p>Otherwise, download VS Code for your operating system.</p> <p>Once installed, open VS Code and enter <code>Ctrl</code>/<code>Cmd</code> + <code>P</code> to open the VS Code Quick Open pane. Then enter:</p> <pre><code>ext install ms-vscode-remote.remote-containers\n</code></pre> <p><code>ms-vscode-remote.remote-containers</code> is the Extension ID of the Dev Containers extension from Microsoft.</p>"},{"location":"development/#get-the-project-code","title":"Get the project code","text":"<p>Use Git to clone the repository to your local machine:</p> <pre><code>git clone https://github.com/compilerla/pems.git\n</code></pre> <p>Then change into the <code>pems</code> directory and create an environment file from the sample:</p> <pre><code>cd pems\ncp .env.sample .env\n</code></pre> <p>Feel free to inspect the environment file, but leave the defaults for now.</p>"},{"location":"development/#run-the-build-script","title":"Run the build script","text":"<p>This builds the runtime and devcontainer images:</p> <pre><code>bin/build.sh\n</code></pre>"},{"location":"development/#run-the-application","title":"Run the application","text":"<p>Start the application service with Compose:</p> <pre><code>docker compose up -d web\n</code></pre> <p>Start the streamlit application with Compose:</p> <pre><code>docker compose up streamlit\n</code></pre> <p>The <code>-d</code> flag starts the service in \u201cdetatched\u201d mode, so your terminal is still available for additional commands. Without this flag, your terminal attaches to the service container\u2019s standard output.</p> <p>The application is now running on <code>http://localhost:8000</code>.</p> <p>Stop the running service with Compose:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"development/#open-the-project-in-a-vs-code-devcontainer","title":"Open the project in a VS Code devcontainer","text":"<p>Still in your terminal, enter the following command to open the project in VS Code:</p> <pre><code>code .\n</code></pre> <p>Once the project is loaded in VS Code, you should see a notification pop-up that will ask if you want to reopen the project in a devcontainer.</p> <p>If you don\u2019t see this notification, or if you dismissed it, use the VS Code Quick Open pane with <code>Ctrl</code>/<code>Cmd</code> + <code>P</code> and enter:</p> <pre><code>&gt; Dev Containers: Rebuild and Reopen in Container\n</code></pre> <p>The VS Code window will reload into the devcontainer.</p> <p>Once loaded, hit <code>F5</code> to start the application in debug mode. The application is now running on <code>http://localhost:8001</code>.</p>"},{"location":"development/#explore-the-devcontainer","title":"Explore the devcontainer","text":"<p>This section describes other areas to explore within the VS Code devcontainer.</p>"},{"location":"development/#debugger","title":"Debugger","text":"<p>Open a Python file in the <code>pems/</code> directory and add a breakpoint by clicking the space next to a line number, leaving a small red circle where you clicked.</p> <p>Step through the running application on <code>http://localhost:8000</code> to trigger the code path you selected. Execution is paused and VS Code allows you to inspect the runtime environment, context, etc.</p>"},{"location":"development/#integrated-terminal","title":"Integrated terminal","text":"<p>Press <code>Ctrl</code> + <code>~</code> to bring up the integrated <code>TERMINAL</code> window. You are now in a <code>bash</code> terminal running inside the context of the devcontainer.</p>"},{"location":"development/#docs-site","title":"Docs site","text":"<p>Open the <code>PORTS</code> tab to see port bindings for additional services. Look for the <code>Forwarded Address</code> of the <code>docs</code> service and click to open the docs site in your browser, running on <code>localhost</code>.</p> <p>Edit the documentation files in VS Code, and once saved, the local docs site will rebuild with the changes.</p>"},{"location":"development/#test-runner","title":"Test runner","text":"<p>Use the VS Code Quick Open pane with <code>Ctrl</code>/<code>Cmd</code> + <code>P</code> and enter:</p> <pre><code>&gt; Testing: Focus\n</code></pre> <p>To focus on the <code>Testing</code> pane on the left side. Click the play button to run the unit tests.</p>"},{"location":"development/#work-with-the-cloud-infrastructure","title":"Work with the Cloud infrastructure","text":"<p>You can work on the app\u2019s Cloud infrastructure in the dev container by using the <code>aws</code> and <code>copilot</code> AWS CLIs. Successfully running the commands requires the container\u2019s host to be configured for authentication with IAM Identity Center by running <code>aws configure sso</code> and going through the setup. You can use the following settings:</p> <ul> <li><code>SSO session name (Recommended): pems</code></li> <li><code>SSO start URL [None]: url_provided_by_caltrans</code></li> <li><code>SSO region [None]: us-west-2</code></li> <li><code>SSO registration scopes [None]:</code></li> <li><code>Default client Region [None]: us-west-2</code></li> <li><code>CLI default output format (json if not specified) [None]:</code></li> <li><code>Profile name [123456789011_ReadOnly]: pems</code></li> </ul> <p>An active SSO session must be available to run the AWS commands, if it is not, run <code>aws sso login</code> inside the container to start a session.</p> <p>Running thes commands in the dev container is made possible by the mapping defined in the compose file that maps the host\u2019s AWS credentials folder to the dev container at <code>/home/caltrans/.aws</code>. For convenience, you can also set the default AWS profile that will be used in the dev container to <code>pems</code> as shown in <code>.env.sample</code>.</p>"},{"location":"development/#useful-commands","title":"Useful commands","text":"<p>Deploying</p> <ul> <li><code>copilot svc deploy -n service-name</code> to deploy a service (local code and configuration) that has already been initialized</li> </ul> <p>Monitoring</p> <ul> <li><code>copilot svc show -n service-name</code> to see the environment variables and other information associated with a service</li> <li><code>copilot svc logs -n service-name --since 1h</code> to see the logs for the past hour of a deployed service</li> <li><code>copilot svc exec -a app-name -e environment-name -n service-name</code> to gain shell access to a running container</li> </ul>"},{"location":"development/pems_data/","title":"Introduction","text":"<p>The <code>pems_data</code> library provides a standardized, efficient interface for accessing Caltrans PeMS data within the project. It handles fetching data from the primary S3 data source and leverages a Redis-based caching layer to optimize performance for repeated requests.</p> <p>This guide covers the specific setup and usage patterns for this library. For general development environment setup, please see the main Getting started with development guide.</p>"},{"location":"development/pems_data/#prerequisites","title":"Prerequisites","text":"<p>Before using the library, ensure your environment is configured correctly.</p>"},{"location":"development/pems_data/#aws-credentials","title":"AWS credentials","text":"<p>The library\u2019s S3 data source requires AWS credentials to be available. The devcontainer is configured to use your host machine\u2019s AWS configuration. For details on setting this up via <code>aws configure sso</code>, please refer to the Work with the Cloud infrastructure section in the main development guide.</p>"},{"location":"development/pems_data/#redis-connection","title":"Redis connection","text":"<p>A running Redis instance is required for the caching layer to function. The connection is configured with the following environment variables, which you can set in the <code>.env</code> file at the root of the project:</p> <pre><code># The hostname for the Redis server\nREDIS_HOSTNAME=redis\n\n# The port for the Redis server.\nREDIS_PORT=6379\n</code></pre> <p>When running locally in the devcontainer, a <code>redis</code> service is started by Compose automatically.</p>"},{"location":"development/pems_data/#architecture-key-concepts","title":"Architecture &amp; Key concepts","text":"<p>The library is built around a few core components that work together to provide a simple data access experience.</p> <ul> <li><code>ServiceFactory</code>: This is the primary entry point for using the library. It is a factory class that instantiates and wires together all the necessary dependencies, such as the data sources and caching clients.</li> </ul> <ul> <li>Services: Services offer a high-level API for fetching specific, business-relevant data. For example, the <code>StationsService</code> has methods to get all station metadata for a given district or to retrieve 5-minute aggregated data for a specific station.</li> </ul> <ul> <li>Caching layer: To minimize latency and load on the data source, the library uses a caching decorator by default. When a data request is made, this layer first checks the Redis cache for the requested data. If the data is not found (a cache miss), it retrieves the data from the underlying S3 source and stores it in the cache for future requests.</li> </ul> <ul> <li>Data sources: The underlying data source reads data directly from Parquet files stored in the Caltrans S3 bucket.</li> </ul>"},{"location":"development/pems_data/#basic-usage","title":"Basic usage","text":"<p>Using the library involves creating the factory, getting a service, and calling a data-fetching method. The factory handles the underlying complexity of connecting to the data source and cache.</p> <pre><code>from pems_data import ServiceFactory\n\n# 1. Create the factory. This wires up all dependencies.\nfactory = ServiceFactory()\n\n# 2. Request a pre-configured service.\nstations_service = factory.stations_service()\n\n# 3. Use the service to fetch data as a pandas DataFrame.\n# This call will attempt to read from the cache first before\n# falling back to the S3 data source.\ndistrict_7_metadata = stations_service.get_district_metadata(district_number=\"7\")\n\nprint(\"Successfully fetched metadata for District 7:\")\nprint(district_7_metadata.head())\n</code></pre>"},{"location":"development/pems_data/cli/","title":"<code>pems-cache</code> CLI","text":"<p>The <code>pems_data</code> package includes <code>pems-cache</code>, a simple command-line tool for interacting directly with the Redis cache. It\u2019s useful for debugging cache issues or manually inspecting and setting values.</p>"},{"location":"development/pems_data/cli/#commands","title":"Commands","text":"<p>The CLI supports three main operations:</p> <ul> <li><code>check</code></li> <li><code>get</code></li> <li><code>set</code></li> </ul> <p>If you run <code>pems-cache</code> with no operation, it defaults to <code>check</code>.</p>"},{"location":"development/pems_data/cli/#check","title":"<code>check</code>","text":"<p>Verifies that a connection to the Redis server can be established and that the cache is responsive.</p>"},{"location":"development/pems_data/cli/#usage","title":"Usage","text":"<pre><code>pems-cache check\n</code></pre>"},{"location":"development/pems_data/cli/#example-output","title":"Example output","text":"<pre><code>$ pems-cache check\ncache is available: True\n</code></pre>"},{"location":"development/pems_data/cli/#get","title":"<code>get</code>","text":"<p>Retrieves and displays a value from the cache based on its key. The <code>--key</code> (or <code>-k</code>) argument is required.</p>"},{"location":"development/pems_data/cli/#usage_1","title":"Usage","text":"<pre><code>pems-cache get --key &lt;cache-key&gt;\n</code></pre>"},{"location":"development/pems_data/cli/#example-output_1","title":"Example output","text":"<pre><code>$ pems-cache get --key \"stations:metadata:district:7\"\n[stations:metadata:district:7]: b'\\x01\\x00\\x00\\x00\\xff\\xff...'\n</code></pre>"},{"location":"development/pems_data/cli/#set","title":"<code>set</code>","text":"<p>Sets a string value for a given key in the cache. Both the <code>--key</code> (<code>-k</code>) and <code>--value</code> (<code>-v</code>) arguments are required.</p>"},{"location":"development/pems_data/cli/#usage_2","title":"Usage","text":"<pre><code>pems-cache set --key &lt;cache-key&gt; --value &lt;cache-value&gt;\n</code></pre>"},{"location":"development/pems_data/cli/#example-output_2","title":"Example output","text":"<pre><code>$ pems-cache set -k \"my:test:key\" -v \"hello from the cli\"\n[my:test:key] = 'hello from the cli'\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/","title":"Caching layer","text":"<p>The caching layer wraps a backing redis service and provides a simple, focused interface to its usage.</p>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.cache","title":"<code>pems_data.cache</code>","text":"<p>The primary caching interface for <code>pems_data</code>.</p>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.cache.Cache","title":"<code>Cache</code>","text":"<p>Basic wrapper for a cache backend.</p> Source code in <code>pems_data/cache.py</code> <pre><code>class Cache:\n    \"\"\"Basic wrapper for a cache backend.\"\"\"\n\n    @classmethod\n    def build_key(cls, *args: Any) -&gt; str:\n        \"\"\"Build a standard cache key from the given parts.\n\n        Args:\n            *args (tuple[Any]): The individual parts that make up the key\n\n        Returns:\n            value (str): A standard representation of the parts for use in a cache key.\n        \"\"\"\n        return \":\".join([str(a).lower() for a in args])\n\n    def __init__(self, host: str = None, port: int = None):\n        \"\"\"Create a new instance of the Cache interface.\n\n        Args:\n            host (str): (Optional) The hostname of the cache backend\n            port (int): (Optional) The port to connect on the cache backend\n        \"\"\"\n\n        self.host = host\n        self.port = port\n        self.c = None\n\n    def _connect(self) -&gt; None:\n        \"\"\"Establish a connection to the cache backend if necessary.\"\"\"\n        if not isinstance(self.c, redis.Redis):\n            self.c = redis_connection(self.host, self.port)\n\n    def is_available(self) -&gt; bool:\n        \"\"\"Return a bool indicating if the cache backend is available or not.\n\n        Returns:\n            value (bool): True if the connection and backend is available; False otherwise\n        \"\"\"\n        self._connect()\n        available = self.c and self.c.ping() is True\n        logger.debug(f\"cache is available: {available}\")\n        return available\n\n    def get(self, key: str, mutate_func: Callable[[Any], Any] = None) -&gt; Any | None:\n        \"\"\"Get a raw value from the cache, or None if the key doesn't exist.\n\n        Args:\n            key (str): The item's cache key\n            mutate_func (callable): (Optional) If provided, call this on the cached value and return its result\n\n        Returns:\n            value (Any | None): The value from the cache, optionally mutated by mutate_func, or None.\n        \"\"\"\n        if self.is_available():\n            logger.debug(f\"read from cache: {key}\")\n            value = self.c.get(key)\n            if value and mutate_func:\n                logger.debug(f\"mutating cached value: {key}\")\n                return mutate_func(value)\n            return value\n        logger.warning(f\"cache unavailable to get: {key}\")\n        return None\n\n    def get_df(self, key: str) -&gt; pd.DataFrame:\n        \"\"\"Get a DataFrame from the cache, or an empty DataFrame if the key wasn't found.\n\n        Args:\n            key (str): The item's cache key\n\n        Returns:\n            value (pandas.DataFrame): The DataFrame materialized from the cache, or an empty DataFrame if the key wasn't found.\n        \"\"\"\n        return self.get(key, mutate_func=arrow_bytes_to_df)\n\n    def set(self, key: str, value: Any, ttl: int = None, mutate_func: Callable[[Any], Any] = None) -&gt; None:\n        \"\"\"Set a value in the cache, with an optional TTL (seconds until expiration).\n\n        Args:\n            key (str): The item's cache key\n            value (Any): The item's value to store in the cache\n            ttl (int): (Optional) Seconds until expiration\n            mutate_func (callable): (Optional) If provided, call this on the value and insert the result in the cache\n        \"\"\"\n        if self.is_available():\n            if mutate_func:\n                logger.debug(f\"mutating value for cache: {key}\")\n                value = mutate_func(value)\n            logger.debug(f\"store in cache: {key}\")\n            self.c.set(key, value, ex=ttl)\n        else:\n            logger.warning(f\"cache unavailable to set: {key}\")\n\n    def set_df(self, key: str, value: pd.DataFrame, ttl: int = None) -&gt; None:\n        \"\"\"Set a DataFrame in the cache, with an optional TTL (seconds until expiration).\n\n        Args:\n            key (str): The item's cache key\n            value (Any): The DataFrame to store in the cache\n            ttl (int): (Optional) Seconds until expiration\n        \"\"\"\n        self.set(key, value, ttl=ttl, mutate_func=df_to_arrow_bytes)\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.cache.Cache.__init__","title":"<code>__init__(host=None, port=None)</code>","text":"<p>Create a new instance of the Cache interface.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>(Optional) The hostname of the cache backend</p> <code>None</code> <code>port</code> <code>int</code> <p>(Optional) The port to connect on the cache backend</p> <code>None</code> Source code in <code>pems_data/cache.py</code> <pre><code>def __init__(self, host: str = None, port: int = None):\n    \"\"\"Create a new instance of the Cache interface.\n\n    Args:\n        host (str): (Optional) The hostname of the cache backend\n        port (int): (Optional) The port to connect on the cache backend\n    \"\"\"\n\n    self.host = host\n    self.port = port\n    self.c = None\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.cache.Cache.build_key","title":"<code>build_key(*args)</code>  <code>classmethod</code>","text":"<p>Build a standard cache key from the given parts.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple[Any]</code> <p>The individual parts that make up the key</p> <code>()</code> <p>Returns:</p> Name Type Description <code>value</code> <code>str</code> <p>A standard representation of the parts for use in a cache key.</p> Source code in <code>pems_data/cache.py</code> <pre><code>@classmethod\ndef build_key(cls, *args: Any) -&gt; str:\n    \"\"\"Build a standard cache key from the given parts.\n\n    Args:\n        *args (tuple[Any]): The individual parts that make up the key\n\n    Returns:\n        value (str): A standard representation of the parts for use in a cache key.\n    \"\"\"\n    return \":\".join([str(a).lower() for a in args])\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.cache.Cache.get","title":"<code>get(key, mutate_func=None)</code>","text":"<p>Get a raw value from the cache, or None if the key doesn\u2019t exist.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The item\u2019s cache key</p> required <code>mutate_func</code> <code>callable</code> <p>(Optional) If provided, call this on the cached value and return its result</p> <code>None</code> <p>Returns:</p> Name Type Description <code>value</code> <code>Any | None</code> <p>The value from the cache, optionally mutated by mutate_func, or None.</p> Source code in <code>pems_data/cache.py</code> <pre><code>def get(self, key: str, mutate_func: Callable[[Any], Any] = None) -&gt; Any | None:\n    \"\"\"Get a raw value from the cache, or None if the key doesn't exist.\n\n    Args:\n        key (str): The item's cache key\n        mutate_func (callable): (Optional) If provided, call this on the cached value and return its result\n\n    Returns:\n        value (Any | None): The value from the cache, optionally mutated by mutate_func, or None.\n    \"\"\"\n    if self.is_available():\n        logger.debug(f\"read from cache: {key}\")\n        value = self.c.get(key)\n        if value and mutate_func:\n            logger.debug(f\"mutating cached value: {key}\")\n            return mutate_func(value)\n        return value\n    logger.warning(f\"cache unavailable to get: {key}\")\n    return None\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.cache.Cache.get_df","title":"<code>get_df(key)</code>","text":"<p>Get a DataFrame from the cache, or an empty DataFrame if the key wasn\u2019t found.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The item\u2019s cache key</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>DataFrame</code> <p>The DataFrame materialized from the cache, or an empty DataFrame if the key wasn\u2019t found.</p> Source code in <code>pems_data/cache.py</code> <pre><code>def get_df(self, key: str) -&gt; pd.DataFrame:\n    \"\"\"Get a DataFrame from the cache, or an empty DataFrame if the key wasn't found.\n\n    Args:\n        key (str): The item's cache key\n\n    Returns:\n        value (pandas.DataFrame): The DataFrame materialized from the cache, or an empty DataFrame if the key wasn't found.\n    \"\"\"\n    return self.get(key, mutate_func=arrow_bytes_to_df)\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.cache.Cache.is_available","title":"<code>is_available()</code>","text":"<p>Return a bool indicating if the cache backend is available or not.</p> <p>Returns:</p> Name Type Description <code>value</code> <code>bool</code> <p>True if the connection and backend is available; False otherwise</p> Source code in <code>pems_data/cache.py</code> <pre><code>def is_available(self) -&gt; bool:\n    \"\"\"Return a bool indicating if the cache backend is available or not.\n\n    Returns:\n        value (bool): True if the connection and backend is available; False otherwise\n    \"\"\"\n    self._connect()\n    available = self.c and self.c.ping() is True\n    logger.debug(f\"cache is available: {available}\")\n    return available\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.cache.Cache.set","title":"<code>set(key, value, ttl=None, mutate_func=None)</code>","text":"<p>Set a value in the cache, with an optional TTL (seconds until expiration).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The item\u2019s cache key</p> required <code>value</code> <code>Any</code> <p>The item\u2019s value to store in the cache</p> required <code>ttl</code> <code>int</code> <p>(Optional) Seconds until expiration</p> <code>None</code> <code>mutate_func</code> <code>callable</code> <p>(Optional) If provided, call this on the value and insert the result in the cache</p> <code>None</code> Source code in <code>pems_data/cache.py</code> <pre><code>def set(self, key: str, value: Any, ttl: int = None, mutate_func: Callable[[Any], Any] = None) -&gt; None:\n    \"\"\"Set a value in the cache, with an optional TTL (seconds until expiration).\n\n    Args:\n        key (str): The item's cache key\n        value (Any): The item's value to store in the cache\n        ttl (int): (Optional) Seconds until expiration\n        mutate_func (callable): (Optional) If provided, call this on the value and insert the result in the cache\n    \"\"\"\n    if self.is_available():\n        if mutate_func:\n            logger.debug(f\"mutating value for cache: {key}\")\n            value = mutate_func(value)\n        logger.debug(f\"store in cache: {key}\")\n        self.c.set(key, value, ex=ttl)\n    else:\n        logger.warning(f\"cache unavailable to set: {key}\")\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.cache.Cache.set_df","title":"<code>set_df(key, value, ttl=None)</code>","text":"<p>Set a DataFrame in the cache, with an optional TTL (seconds until expiration).</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The item\u2019s cache key</p> required <code>value</code> <code>Any</code> <p>The DataFrame to store in the cache</p> required <code>ttl</code> <code>int</code> <p>(Optional) Seconds until expiration</p> <code>None</code> Source code in <code>pems_data/cache.py</code> <pre><code>def set_df(self, key: str, value: pd.DataFrame, ttl: int = None) -&gt; None:\n    \"\"\"Set a DataFrame in the cache, with an optional TTL (seconds until expiration).\n\n    Args:\n        key (str): The item's cache key\n        value (Any): The DataFrame to store in the cache\n        ttl (int): (Optional) Seconds until expiration\n    \"\"\"\n    self.set(key, value, ttl=ttl, mutate_func=df_to_arrow_bytes)\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.cache.redis_connection","title":"<code>redis_connection(host=None, port=None, **kwargs)</code>","text":"<p>Try to create a new connection to a redis backend. Return None if the connection fails.</p> <p>Uses the <code>REDIS_HOSTNAME</code> and <code>REDIS_PORT</code> environment variables as fallback.</p> <p>Parameters:</p> Name Type Description Default <code>host</code> <code>str</code> <p>(Optional) The redis hostname</p> <code>None</code> <code>port</code> <code>int</code> <p>(Optional) The port to connect on</p> <code>None</code> <p>Returns:</p> Name Type Description <code>value</code> <code>Redis</code> <p>A Redis instance connected to <code>host:port</code>, or None if the connection failed.</p> Source code in <code>pems_data/cache.py</code> <pre><code>def redis_connection(host: str = None, port: int = None, **kwargs: dict[str, Any]) -&gt; redis.Redis | None:\n    \"\"\"Try to create a new connection to a redis backend. Return None if the connection fails.\n\n    Uses the `REDIS_HOSTNAME` and `REDIS_PORT` environment variables as fallback.\n\n    Args:\n        host (str): (Optional) The redis hostname\n        port (int): (Optional) The port to connect on\n\n    Returns:\n        value (redis.Redis): A Redis instance connected to `host:port`, or None if the connection failed.\n    \"\"\"\n\n    host = host or os.environ.get(\"REDIS_HOSTNAME\", \"redis\")\n    port = int(port or os.environ.get(\"REDIS_PORT\", \"6379\"))\n\n    logger.debug(f\"connecting to redis @ {host}:{port}\")\n\n    kwargs[\"host\"] = host\n    kwargs[\"port\"] = port\n\n    try:\n        return redis.Redis(**kwargs)\n    except redis.ConnectionError as ce:\n        logger.error(f\"connection failed for redis @ {host}:{port}\", exc_info=ce)\n        return None\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.serialization","title":"<code>pems_data.serialization</code>","text":"<p>Helpers for serializing specific data types e.g. for storing/retrieving from a cache.</p>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.serialization.arrow_bytes_to_df","title":"<code>arrow_bytes_to_df(arrow_buffer)</code>","text":"<p>Deserializes Arrow IPC format bytes back to a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>arrow_buffer</code> <code>bytes</code> <p>A buffer of Arrow IPC bytes representing a DataFrame</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>DataFrame</code> <p>The DataFrame deserialized from the buffer, or an empty DataFrame if the buffer was empty.</p> Source code in <code>pems_data/serialization.py</code> <pre><code>def arrow_bytes_to_df(arrow_buffer: bytes) -&gt; pd.DataFrame:\n    \"\"\"Deserializes Arrow IPC format bytes back to a DataFrame.\n\n    Args:\n        arrow_buffer (bytes): A buffer of Arrow IPC bytes representing a DataFrame\n\n    Returns:\n        value (pandas.DataFrame): The DataFrame deserialized from the buffer, or an empty DataFrame if the buffer was empty.\n    \"\"\"\n    if not arrow_buffer:\n        return pd.DataFrame()\n    # deserialize the Arrow IPC stream\n    with pa.BufferReader(arrow_buffer) as buffer:\n        # the reader reconstructs the Arrow Table from the buffer\n        reader = ipc.RecordBatchStreamReader(buffer)\n        arrow_table = reader.read_all()\n    return arrow_table.to_pandas()\n</code></pre>"},{"location":"development/pems_data/reference/caching-layer/#pems_data.serialization.df_to_arrow_bytes","title":"<code>df_to_arrow_bytes(df)</code>","text":"<p>Serializes a DataFrame to Arrow IPC format bytes.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to serialize</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>bytes</code> <p>The Arrow IPC format bytes representation of the DataFrame.</p> Source code in <code>pems_data/serialization.py</code> <pre><code>def df_to_arrow_bytes(df: pd.DataFrame) -&gt; bytes:\n    \"\"\"Serializes a DataFrame to Arrow IPC format bytes.\n\n    Args:\n        df (pandas.DataFrame): The DataFrame to serialize\n\n    Returns:\n        value (bytes): The Arrow IPC format bytes representation of the DataFrame.\n    \"\"\"\n    if df.empty:\n        return b\"\"\n    # convert DataFrame to an Arrow Table\n    arrow_table = pa.Table.from_pandas(df, preserve_index=False)\n    # serialize the Arrow Table to bytes using the IPC stream format\n    sink = pa.BufferOutputStream()\n    with ipc.RecordBatchStreamWriter(sink, arrow_table.schema) as writer:\n        writer.write_table(arrow_table)\n    # get the buffer from the stream\n    return sink.getvalue().to_pybytes()\n</code></pre>"},{"location":"development/pems_data/reference/data-sources/","title":"Data sources","text":"<p>The data source components are responsible for the actual reading of data (the \u201chow\u201d). The design uses an abstract interface, <code>IDataSource</code>, to define a standard contract for any data source, making it easy to swap and compose implementations.</p>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.IDataSource","title":"<code>pems_data.sources.IDataSource</code>","text":"<p>               Bases: <code>ABC</code></p> <p>An abstract interface for a generic data source.</p> Source code in <code>pems_data/sources/__init__.py</code> <pre><code>class IDataSource(ABC):\n    \"\"\"An abstract interface for a generic data source.\"\"\"\n\n    @abstractmethod\n    def read(self, identifier: str, **kwargs: dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"\n        Reads data identified by a generic identifier from the source.\n\n        Args:\n            identifier (str): The unique identifier for the data, e.g., an S3 key, a database table name, etc.\n            **kwargs (dict[str, Any]): Additional arguments for the underlying read operation, such as 'columns' or 'filters'.\n\n        Returns:\n            value (pandas.DataFrame): A DataFrame of data from the source for the given identifier.\n        \"\"\"\n        raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.IDataSource.read","title":"<code>read(identifier, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Reads data identified by a generic identifier from the source.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The unique identifier for the data, e.g., an S3 key, a database table name, etc.</p> required <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional arguments for the underlying read operation, such as \u2018columns\u2019 or \u2018filters\u2019.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>value</code> <code>DataFrame</code> <p>A DataFrame of data from the source for the given identifier.</p> Source code in <code>pems_data/sources/__init__.py</code> <pre><code>@abstractmethod\ndef read(self, identifier: str, **kwargs: dict[str, Any]) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads data identified by a generic identifier from the source.\n\n    Args:\n        identifier (str): The unique identifier for the data, e.g., an S3 key, a database table name, etc.\n        **kwargs (dict[str, Any]): Additional arguments for the underlying read operation, such as 'columns' or 'filters'.\n\n    Returns:\n        value (pandas.DataFrame): A DataFrame of data from the source for the given identifier.\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.s3.S3DataSource","title":"<code>pems_data.sources.s3.S3DataSource</code>","text":"<p>               Bases: <code>IDataSource</code></p> <p>A data source for fetching data from an S3 bucket.</p> Source code in <code>pems_data/sources/s3.py</code> <pre><code>class S3DataSource(IDataSource):\n    \"\"\"A data source for fetching data from an S3 bucket.\"\"\"\n\n    @property\n    def default_bucket(self) -&gt; str:\n        \"\"\"\n        Returns:\n            value (str): The value from the `S3_BUCKET_NAME` environment variable, or the Caltrans PeMS prod mart bucket name.\n        \"\"\"\n        return os.environ.get(\"S3_BUCKET_NAME\", \"caltrans-pems-prd-us-west-2-marts\")\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Returns:\n            value (str): The name of this bucket instance.\n        \"\"\"\n        return self._name\n\n    def __init__(self, name: str = None):\n        \"\"\"Initialize a new S3DataSource.\n\n        Args:\n            name (str): (Optional) The name of the S3 bucket to source from.\n        \"\"\"\n        self._client = boto3.client(\"s3\")\n        self._name = name or self.default_bucket\n\n    def get_prefixes(\n        self,\n        filter_pattern: re.Pattern = re.compile(\".+\"),\n        initial_prefix: str = \"\",\n        match_func: Callable[[re.Match], str] = None,\n    ) -&gt; list:\n        \"\"\"\n        Lists available object prefixes, optionally filtered by an initial prefix.\n\n        When a match is found, if match_func exists, add its result to the output list. Otherwise add the entire match.\n\n        Args:\n            filter_pattern (re.Pattern): A regular expression used to match object prefixes\n            initial_prefix (str): The initial prefix to start the search from\n            match_func (Callable[[re.Match], str]): A callable used to extract data from prefix matches\n\n        Returns:\n            value (list): A sorted list of unique prefixes that matched the pattern.\n        \"\"\"\n\n        s3_keys = self._client.list_objects(Bucket=self.name, Prefix=initial_prefix)\n\n        result = set()\n\n        for item in s3_keys[\"Contents\"]:\n            s3_path = item[\"Key\"]\n            match = re.search(filter_pattern, s3_path)\n            if match:\n                if match_func:\n                    result.add(match_func(match))\n                else:\n                    result.add(match.group(0))\n\n        return sorted(result)\n\n    def read(\n        self, *args: str, path: str = None, columns: list = None, filters: list = None, **kwargs: dict[str, Any]\n    ) -&gt; pd.DataFrame:\n        \"\"\"Reads data from the S3 path into a pandas DataFrame. Extra kwargs are passed along to `pandas.read_parquet()`.\n\n        Args:\n            *args (tuple[str]): One or more path relative path components for the data file\n            path (str): The absolute S3 URL path to a data file; using `path` overrides any relative path components provided\n            columns (list[str]): If not None, only these columns will be read from the file\n            filters (list[tuple] | list[list[tuple]]): To filter out data. Filter syntax: `[[(column, op, val), ...],...]`\n            **kwargs (dict[str, Any]): Extra kwargs to pass to `pandas.read_parquet()`\n\n        Returns:\n            value (pandas.DataFrame): A DataFrame of data read from the source path.\n        \"\"\"\n        path = path or self.url(*args)\n        return pd.read_parquet(path, columns=columns, filters=filters, **kwargs)\n\n    def url(self, *args: str) -&gt; str:\n        \"\"\"Build an absolute S3 URL to this bucket, with optional path segments.\n\n        Args:\n            *args (tuple[str]): The components of the S3 path.\n\n        Returns:\n            value (str): An absolute `s3://` URL for this bucket and the path.\n        \"\"\"\n        parts = [f\"s3://{self.name}\"]\n        parts.extend(args)\n        return \"/\".join(parts)\n</code></pre>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.s3.S3DataSource.default_bucket","title":"<code>default_bucket</code>  <code>property</code>","text":"<p>Returns:</p> Name Type Description <code>value</code> <code>str</code> <p>The value from the <code>S3_BUCKET_NAME</code> environment variable, or the Caltrans PeMS prod mart bucket name.</p>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.s3.S3DataSource.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns:</p> Name Type Description <code>value</code> <code>str</code> <p>The name of this bucket instance.</p>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.s3.S3DataSource.__init__","title":"<code>__init__(name=None)</code>","text":"<p>Initialize a new S3DataSource.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>(Optional) The name of the S3 bucket to source from.</p> <code>None</code> Source code in <code>pems_data/sources/s3.py</code> <pre><code>def __init__(self, name: str = None):\n    \"\"\"Initialize a new S3DataSource.\n\n    Args:\n        name (str): (Optional) The name of the S3 bucket to source from.\n    \"\"\"\n    self._client = boto3.client(\"s3\")\n    self._name = name or self.default_bucket\n</code></pre>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.s3.S3DataSource.get_prefixes","title":"<code>get_prefixes(filter_pattern=re.compile('.+'), initial_prefix='', match_func=None)</code>","text":"<p>Lists available object prefixes, optionally filtered by an initial prefix.</p> <p>When a match is found, if match_func exists, add its result to the output list. Otherwise add the entire match.</p> <p>Parameters:</p> Name Type Description Default <code>filter_pattern</code> <code>Pattern</code> <p>A regular expression used to match object prefixes</p> <code>compile('.+')</code> <code>initial_prefix</code> <code>str</code> <p>The initial prefix to start the search from</p> <code>''</code> <code>match_func</code> <code>Callable[[Match], str]</code> <p>A callable used to extract data from prefix matches</p> <code>None</code> <p>Returns:</p> Name Type Description <code>value</code> <code>list</code> <p>A sorted list of unique prefixes that matched the pattern.</p> Source code in <code>pems_data/sources/s3.py</code> <pre><code>def get_prefixes(\n    self,\n    filter_pattern: re.Pattern = re.compile(\".+\"),\n    initial_prefix: str = \"\",\n    match_func: Callable[[re.Match], str] = None,\n) -&gt; list:\n    \"\"\"\n    Lists available object prefixes, optionally filtered by an initial prefix.\n\n    When a match is found, if match_func exists, add its result to the output list. Otherwise add the entire match.\n\n    Args:\n        filter_pattern (re.Pattern): A regular expression used to match object prefixes\n        initial_prefix (str): The initial prefix to start the search from\n        match_func (Callable[[re.Match], str]): A callable used to extract data from prefix matches\n\n    Returns:\n        value (list): A sorted list of unique prefixes that matched the pattern.\n    \"\"\"\n\n    s3_keys = self._client.list_objects(Bucket=self.name, Prefix=initial_prefix)\n\n    result = set()\n\n    for item in s3_keys[\"Contents\"]:\n        s3_path = item[\"Key\"]\n        match = re.search(filter_pattern, s3_path)\n        if match:\n            if match_func:\n                result.add(match_func(match))\n            else:\n                result.add(match.group(0))\n\n    return sorted(result)\n</code></pre>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.s3.S3DataSource.read","title":"<code>read(*args, path=None, columns=None, filters=None, **kwargs)</code>","text":"<p>Reads data from the S3 path into a pandas DataFrame. Extra kwargs are passed along to <code>pandas.read_parquet()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple[str]</code> <p>One or more path relative path components for the data file</p> <code>()</code> <code>path</code> <code>str</code> <p>The absolute S3 URL path to a data file; using <code>path</code> overrides any relative path components provided</p> <code>None</code> <code>columns</code> <code>list[str]</code> <p>If not None, only these columns will be read from the file</p> <code>None</code> <code>filters</code> <code>list[tuple] | list[list[tuple]]</code> <p>To filter out data. Filter syntax: <code>[[(column, op, val), ...],...]</code></p> <code>None</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Extra kwargs to pass to <code>pandas.read_parquet()</code></p> <code>{}</code> <p>Returns:</p> Name Type Description <code>value</code> <code>DataFrame</code> <p>A DataFrame of data read from the source path.</p> Source code in <code>pems_data/sources/s3.py</code> <pre><code>def read(\n    self, *args: str, path: str = None, columns: list = None, filters: list = None, **kwargs: dict[str, Any]\n) -&gt; pd.DataFrame:\n    \"\"\"Reads data from the S3 path into a pandas DataFrame. Extra kwargs are passed along to `pandas.read_parquet()`.\n\n    Args:\n        *args (tuple[str]): One or more path relative path components for the data file\n        path (str): The absolute S3 URL path to a data file; using `path` overrides any relative path components provided\n        columns (list[str]): If not None, only these columns will be read from the file\n        filters (list[tuple] | list[list[tuple]]): To filter out data. Filter syntax: `[[(column, op, val), ...],...]`\n        **kwargs (dict[str, Any]): Extra kwargs to pass to `pandas.read_parquet()`\n\n    Returns:\n        value (pandas.DataFrame): A DataFrame of data read from the source path.\n    \"\"\"\n    path = path or self.url(*args)\n    return pd.read_parquet(path, columns=columns, filters=filters, **kwargs)\n</code></pre>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.s3.S3DataSource.url","title":"<code>url(*args)</code>","text":"<p>Build an absolute S3 URL to this bucket, with optional path segments.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple[str]</code> <p>The components of the S3 path.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>value</code> <code>str</code> <p>An absolute <code>s3://</code> URL for this bucket and the path.</p> Source code in <code>pems_data/sources/s3.py</code> <pre><code>def url(self, *args: str) -&gt; str:\n    \"\"\"Build an absolute S3 URL to this bucket, with optional path segments.\n\n    Args:\n        *args (tuple[str]): The components of the S3 path.\n\n    Returns:\n        value (str): An absolute `s3://` URL for this bucket and the path.\n    \"\"\"\n    parts = [f\"s3://{self.name}\"]\n    parts.extend(args)\n    return \"/\".join(parts)\n</code></pre>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.cache.CachingDataSource","title":"<code>pems_data.sources.cache.CachingDataSource</code>","text":"<p>               Bases: <code>IDataSource</code></p> <p>A data source decorator that adds a caching layer to another data source.</p> Source code in <code>pems_data/sources/cache.py</code> <pre><code>class CachingDataSource(IDataSource):\n    \"\"\"A data source decorator that adds a caching layer to another data source.\"\"\"\n\n    @property\n    def cache(self) -&gt; Cache:\n        \"\"\"\n        Returns:\n            value (pems_data.cache.Cache): This data source's underlying Cache instance.\n        \"\"\"\n        return self._cache\n\n    @property\n    def data_source(self) -&gt; IDataSource:\n        \"\"\"\n        Returns:\n            value (pems_data.sources.IDataSource): This data source's underlying data source instance.\n        \"\"\"\n        return self._data_source\n\n    def __init__(self, data_source: IDataSource, cache: Cache):\n        \"\"\"Initialize a new CachingDataSource.\n\n        Args:\n            data_source (pems_data.sources.IDataSource): The underlying data source to use for cache misses\n            cache (pems_data.cache.Cache): The underlying cache to use for get/set operations\n        \"\"\"\n        self._cache = cache\n        self._data_source = data_source\n\n    def read(self, identifier: str, cache_opts: dict[str, Any] = {}, **kwargs: dict[str, Any]) -&gt; pd.DataFrame:\n        \"\"\"\n        Reads data identified by a generic identifier from the source. Tries the cache first, setting on a miss.\n\n        Args:\n            identifier (str): The unique identifier for the data, e.g., an S3 key, a database table name, etc.\n            cache_opts (dict[str, Any]): A dictionary of options for configuring caching of the data\n            **kwargs (dict[str, Any]): Additional arguments for the underlying read operation, such as 'columns' or 'filters'\n\n        Returns:\n            value (pandas.DataFrame): A DataFrame of data read from the cache (or the source), for the given identifier.\n        \"\"\"\n        # use cache key from options, fallback to identifier\n        cache_key = cache_opts.get(\"key\", identifier)\n        ttl = cache_opts.get(\"ttl\")\n\n        # try to get df from cache\n        cached_df = self._cache.get_df(cache_key)\n        if cached_df is not None:\n            return cached_df\n\n        # on miss, call the wrapped source\n        df = self._data_source.read(identifier, **kwargs)\n        # store the result in the cache\n        self._cache.set_df(cache_key, df, ttl=ttl)\n\n        return df\n</code></pre>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.cache.CachingDataSource.cache","title":"<code>cache</code>  <code>property</code>","text":"<p>Returns:</p> Name Type Description <code>value</code> <code>Cache</code> <p>This data source\u2019s underlying Cache instance.</p>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.cache.CachingDataSource.data_source","title":"<code>data_source</code>  <code>property</code>","text":"<p>Returns:</p> Name Type Description <code>value</code> <code>IDataSource</code> <p>This data source\u2019s underlying data source instance.</p>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.cache.CachingDataSource.__init__","title":"<code>__init__(data_source, cache)</code>","text":"<p>Initialize a new CachingDataSource.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>IDataSource</code> <p>The underlying data source to use for cache misses</p> required <code>cache</code> <code>Cache</code> <p>The underlying cache to use for get/set operations</p> required Source code in <code>pems_data/sources/cache.py</code> <pre><code>def __init__(self, data_source: IDataSource, cache: Cache):\n    \"\"\"Initialize a new CachingDataSource.\n\n    Args:\n        data_source (pems_data.sources.IDataSource): The underlying data source to use for cache misses\n        cache (pems_data.cache.Cache): The underlying cache to use for get/set operations\n    \"\"\"\n    self._cache = cache\n    self._data_source = data_source\n</code></pre>"},{"location":"development/pems_data/reference/data-sources/#pems_data.sources.cache.CachingDataSource.read","title":"<code>read(identifier, cache_opts={}, **kwargs)</code>","text":"<p>Reads data identified by a generic identifier from the source. Tries the cache first, setting on a miss.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The unique identifier for the data, e.g., an S3 key, a database table name, etc.</p> required <code>cache_opts</code> <code>dict[str, Any]</code> <p>A dictionary of options for configuring caching of the data</p> <code>{}</code> <code>**kwargs</code> <code>dict[str, Any]</code> <p>Additional arguments for the underlying read operation, such as \u2018columns\u2019 or \u2018filters\u2019</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>value</code> <code>DataFrame</code> <p>A DataFrame of data read from the cache (or the source), for the given identifier.</p> Source code in <code>pems_data/sources/cache.py</code> <pre><code>def read(self, identifier: str, cache_opts: dict[str, Any] = {}, **kwargs: dict[str, Any]) -&gt; pd.DataFrame:\n    \"\"\"\n    Reads data identified by a generic identifier from the source. Tries the cache first, setting on a miss.\n\n    Args:\n        identifier (str): The unique identifier for the data, e.g., an S3 key, a database table name, etc.\n        cache_opts (dict[str, Any]): A dictionary of options for configuring caching of the data\n        **kwargs (dict[str, Any]): Additional arguments for the underlying read operation, such as 'columns' or 'filters'\n\n    Returns:\n        value (pandas.DataFrame): A DataFrame of data read from the cache (or the source), for the given identifier.\n    \"\"\"\n    # use cache key from options, fallback to identifier\n    cache_key = cache_opts.get(\"key\", identifier)\n    ttl = cache_opts.get(\"ttl\")\n\n    # try to get df from cache\n    cached_df = self._cache.get_df(cache_key)\n    if cached_df is not None:\n        return cached_df\n\n    # on miss, call the wrapped source\n    df = self._data_source.read(identifier, **kwargs)\n    # store the result in the cache\n    self._cache.set_df(cache_key, df, ttl=ttl)\n\n    return df\n</code></pre>"},{"location":"development/pems_data/reference/service-factory/","title":"Service factory","text":""},{"location":"development/pems_data/reference/service-factory/#pems_data.ServiceFactory","title":"<code>pems_data.ServiceFactory</code>","text":"<p>A factory class to create and configure various services.</p> <p>Shared dependencies are created once during initialization.</p> Source code in <code>pems_data/__init__.py</code> <pre><code>class ServiceFactory:\n    \"\"\"\n    A factory class to create and configure various services.\n\n    Shared dependencies are created once during initialization.\n    \"\"\"\n\n    @property\n    def cache(self) -&gt; Cache:\n        \"\"\"\n        Returns:\n            value (pems_data.cache.Cache): The shared Cache instance managed by this factory.\n        \"\"\"\n        return self._cache\n\n    @property\n    def s3_source(self) -&gt; S3DataSource:\n        \"\"\"\n        Returns:\n            value (pems_data.sources.s3.S3DataSource): The shared S3DataSource instance managed by this factory.\n        \"\"\"\n        return self._s3_source\n\n    @property\n    def caching_s3_source(self) -&gt; CachingDataSource:\n        \"\"\"\n        Returns:\n            value (pems_data.sources.cache.CachingDataSource): The shared CachingDataSource instance managed by this factory.\n        \"\"\"\n        return self._caching_s3_source\n\n    def __init__(self):\n        \"\"\"Initializes a new ServiceFactory and shared dependencies.\"\"\"\n        self._cache = Cache()\n        self._s3_source = S3DataSource()\n        self._caching_s3_source = CachingDataSource(data_source=self._s3_source, cache=self._cache)\n\n    def stations_service(self) -&gt; StationsService:\n        \"\"\"Creates a fully-configured StationsService.\n\n        Returns:\n            value (pems_data.services.stations.StationsService): A StationsService instance configured by the factory.\n        \"\"\"\n        return StationsService(data_source=self._caching_s3_source)\n</code></pre>"},{"location":"development/pems_data/reference/service-factory/#pems_data.ServiceFactory.cache","title":"<code>cache</code>  <code>property</code>","text":"<p>Returns:</p> Name Type Description <code>value</code> <code>Cache</code> <p>The shared Cache instance managed by this factory.</p>"},{"location":"development/pems_data/reference/service-factory/#pems_data.ServiceFactory.caching_s3_source","title":"<code>caching_s3_source</code>  <code>property</code>","text":"<p>Returns:</p> Name Type Description <code>value</code> <code>CachingDataSource</code> <p>The shared CachingDataSource instance managed by this factory.</p>"},{"location":"development/pems_data/reference/service-factory/#pems_data.ServiceFactory.s3_source","title":"<code>s3_source</code>  <code>property</code>","text":"<p>Returns:</p> Name Type Description <code>value</code> <code>S3DataSource</code> <p>The shared S3DataSource instance managed by this factory.</p>"},{"location":"development/pems_data/reference/service-factory/#pems_data.ServiceFactory.__init__","title":"<code>__init__()</code>","text":"<p>Initializes a new ServiceFactory and shared dependencies.</p> Source code in <code>pems_data/__init__.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes a new ServiceFactory and shared dependencies.\"\"\"\n    self._cache = Cache()\n    self._s3_source = S3DataSource()\n    self._caching_s3_source = CachingDataSource(data_source=self._s3_source, cache=self._cache)\n</code></pre>"},{"location":"development/pems_data/reference/service-factory/#pems_data.ServiceFactory.stations_service","title":"<code>stations_service()</code>","text":"<p>Creates a fully-configured StationsService.</p> <p>Returns:</p> Name Type Description <code>value</code> <code>StationsService</code> <p>A StationsService instance configured by the factory.</p> Source code in <code>pems_data/__init__.py</code> <pre><code>def stations_service(self) -&gt; StationsService:\n    \"\"\"Creates a fully-configured StationsService.\n\n    Returns:\n        value (pems_data.services.stations.StationsService): A StationsService instance configured by the factory.\n    \"\"\"\n    return StationsService(data_source=self._caching_s3_source)\n</code></pre>"},{"location":"development/pems_data/reference/services/","title":"Services","text":"<p>The services represent the business-logic of \u201cwhat\u201d data to fetch for specific use-cases. Services require an underlying data source to perform the actual reading of data.</p>"},{"location":"development/pems_data/reference/services/#pems_data.services.stations.StationsService","title":"<code>pems_data.services.stations.StationsService</code>","text":"<p>Manages fetching of station-related data.</p> Source code in <code>pems_data/services/stations.py</code> <pre><code>class StationsService:\n    \"\"\"Manages fetching of station-related data.\"\"\"\n\n    @property\n    def data_source(self) -&gt; IDataSource:\n        \"\"\"This service's IDataSource instance.\"\"\"\n        return self._ds\n\n    @property\n    def imputation_detector_agg_5min(self) -&gt; str:\n        \"\"\"\n        Returns:\n            value (str): The identifier for the imputation detector 5min aggregation\n        \"\"\"\n        return \"imputation/detector_imputed_agg_five_minutes\"\n\n    @property\n    def metadata_file(self) -&gt; str:\n        \"\"\"\n        Returns:\n            value (str): The identifier for the stations metadata file.\n        \"\"\"\n        return \"geo/current_stations.parquet\"\n\n    def __init__(self, data_source: IDataSource):\n        \"\"\"Initialize a new StationsService.\n\n        Args:\n            data_source (pems_data.sources.IDataSource): The data source responsible for fetching data for this service.\n        \"\"\"\n        self._ds = data_source\n\n    def _build_cache_key(self, *args):\n        return Cache.build_key(\"stations\", *args)\n\n    def get_district_metadata(self, district_number: str) -&gt; pd.DataFrame:\n        \"\"\"Loads metadata for all stations in the selected district from the data source.\n\n        Args:\n            district_number (str): The number of the Caltrans district to load metadata for, e.g. `\"7\"`.\n\n        Returns:\n            value (pandas.DataFrame): The station's data as a DataFrame.\n        \"\"\"\n\n        cache_opts = {\"key\": self._build_cache_key(\"metadata\", \"district\", district_number), \"ttl\": 3600}  # 1 hour\n        columns = [\n            \"STATION_ID\",\n            \"NAME\",\n            \"PHYSICAL_LANES\",\n            \"STATE_POSTMILE\",\n            \"ABSOLUTE_POSTMILE\",\n            \"LATITUDE\",\n            \"LONGITUDE\",\n            \"LENGTH\",\n            \"STATION_TYPE\",\n            \"DISTRICT\",\n            \"FREEWAY\",\n            \"DIRECTION\",\n            \"COUNTY_NAME\",\n            \"CITY_NAME\",\n        ]\n        filters = [(\"DISTRICT\", \"=\", district_number)]\n\n        return self._ds.read(self.metadata_file, cache_opts=cache_opts, columns=columns, filters=filters)\n\n    def get_imputed_agg_5min(self, station_id: str) -&gt; pd.DataFrame:\n        \"\"\"Loads imputed aggregate 5 minute data for a specific station from the data source.\n\n        Args:\n            station_id (str): The identifier for the station/detector to load data, e.g. `\"715898\"`\n\n        Returns:\n            value (pandas.DataFrame): The station's data as a DataFrame.\n        \"\"\"\n\n        cache_opts = {\"key\": self._build_cache_key(\"imputed\", \"agg\", \"5m\", \"station\", station_id), \"ttl\": 3600}  # 1 hour\n        columns = [\n            \"STATION_ID\",\n            \"LANE\",\n            \"SAMPLE_TIMESTAMP\",\n            \"VOLUME_SUM\",\n            \"SPEED_FIVE_MINS\",\n            \"OCCUPANCY_AVG\",\n        ]\n        filters = [(\"STATION_ID\", \"=\", station_id)]\n\n        return self._ds.read(self.imputation_detector_agg_5min, cache_opts=cache_opts, columns=columns, filters=filters)\n</code></pre>"},{"location":"development/pems_data/reference/services/#pems_data.services.stations.StationsService.data_source","title":"<code>data_source</code>  <code>property</code>","text":"<p>This service\u2019s IDataSource instance.</p>"},{"location":"development/pems_data/reference/services/#pems_data.services.stations.StationsService.imputation_detector_agg_5min","title":"<code>imputation_detector_agg_5min</code>  <code>property</code>","text":"<p>Returns:</p> Name Type Description <code>value</code> <code>str</code> <p>The identifier for the imputation detector 5min aggregation</p>"},{"location":"development/pems_data/reference/services/#pems_data.services.stations.StationsService.metadata_file","title":"<code>metadata_file</code>  <code>property</code>","text":"<p>Returns:</p> Name Type Description <code>value</code> <code>str</code> <p>The identifier for the stations metadata file.</p>"},{"location":"development/pems_data/reference/services/#pems_data.services.stations.StationsService.__init__","title":"<code>__init__(data_source)</code>","text":"<p>Initialize a new StationsService.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>IDataSource</code> <p>The data source responsible for fetching data for this service.</p> required Source code in <code>pems_data/services/stations.py</code> <pre><code>def __init__(self, data_source: IDataSource):\n    \"\"\"Initialize a new StationsService.\n\n    Args:\n        data_source (pems_data.sources.IDataSource): The data source responsible for fetching data for this service.\n    \"\"\"\n    self._ds = data_source\n</code></pre>"},{"location":"development/pems_data/reference/services/#pems_data.services.stations.StationsService.get_district_metadata","title":"<code>get_district_metadata(district_number)</code>","text":"<p>Loads metadata for all stations in the selected district from the data source.</p> <p>Parameters:</p> Name Type Description Default <code>district_number</code> <code>str</code> <p>The number of the Caltrans district to load metadata for, e.g. <code>\"7\"</code>.</p> required <p>Returns:</p> Name Type Description <code>value</code> <code>DataFrame</code> <p>The station\u2019s data as a DataFrame.</p> Source code in <code>pems_data/services/stations.py</code> <pre><code>def get_district_metadata(self, district_number: str) -&gt; pd.DataFrame:\n    \"\"\"Loads metadata for all stations in the selected district from the data source.\n\n    Args:\n        district_number (str): The number of the Caltrans district to load metadata for, e.g. `\"7\"`.\n\n    Returns:\n        value (pandas.DataFrame): The station's data as a DataFrame.\n    \"\"\"\n\n    cache_opts = {\"key\": self._build_cache_key(\"metadata\", \"district\", district_number), \"ttl\": 3600}  # 1 hour\n    columns = [\n        \"STATION_ID\",\n        \"NAME\",\n        \"PHYSICAL_LANES\",\n        \"STATE_POSTMILE\",\n        \"ABSOLUTE_POSTMILE\",\n        \"LATITUDE\",\n        \"LONGITUDE\",\n        \"LENGTH\",\n        \"STATION_TYPE\",\n        \"DISTRICT\",\n        \"FREEWAY\",\n        \"DIRECTION\",\n        \"COUNTY_NAME\",\n        \"CITY_NAME\",\n    ]\n    filters = [(\"DISTRICT\", \"=\", district_number)]\n\n    return self._ds.read(self.metadata_file, cache_opts=cache_opts, columns=columns, filters=filters)\n</code></pre>"},{"location":"development/pems_data/reference/services/#pems_data.services.stations.StationsService.get_imputed_agg_5min","title":"<code>get_imputed_agg_5min(station_id)</code>","text":"<p>Loads imputed aggregate 5 minute data for a specific station from the data source.</p> <p>Parameters:</p> Name Type Description Default <code>station_id</code> <code>str</code> <p>The identifier for the station/detector to load data, e.g. <code>\"715898\"</code></p> required <p>Returns:</p> Name Type Description <code>value</code> <code>DataFrame</code> <p>The station\u2019s data as a DataFrame.</p> Source code in <code>pems_data/services/stations.py</code> <pre><code>def get_imputed_agg_5min(self, station_id: str) -&gt; pd.DataFrame:\n    \"\"\"Loads imputed aggregate 5 minute data for a specific station from the data source.\n\n    Args:\n        station_id (str): The identifier for the station/detector to load data, e.g. `\"715898\"`\n\n    Returns:\n        value (pandas.DataFrame): The station's data as a DataFrame.\n    \"\"\"\n\n    cache_opts = {\"key\": self._build_cache_key(\"imputed\", \"agg\", \"5m\", \"station\", station_id), \"ttl\": 3600}  # 1 hour\n    columns = [\n        \"STATION_ID\",\n        \"LANE\",\n        \"SAMPLE_TIMESTAMP\",\n        \"VOLUME_SUM\",\n        \"SPEED_FIVE_MINS\",\n        \"OCCUPANCY_AVG\",\n    ]\n    filters = [(\"STATION_ID\", \"=\", station_id)]\n\n    return self._ds.read(self.imputation_detector_agg_5min, cache_opts=cache_opts, columns=columns, filters=filters)\n</code></pre>"}]}